{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFH9HWw5XGwhVpJkZITYa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caileymm/cuda-by-example-exercises/blob/main/chapter_3_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xenPw-PbJUz_",
        "outputId": "5282627c-aac5-45bd-95ff-61ddedabc2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpj4eo2r80\".\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile book.h\n",
        "/*\n",
        " * Copyright 1993-2010 NVIDIA Corporation.  All rights reserved.\n",
        " *\n",
        " * NVIDIA Corporation and its licensors retain all intellectual property and\n",
        " * proprietary rights in and to this software and related documentation.\n",
        " * Any use, reproduction, disclosure, or distribution of this software\n",
        " * and related documentation without an express license agreement from\n",
        " * NVIDIA Corporation is strictly prohibited.\n",
        " *\n",
        " * Please refer to the applicable NVIDIA end user license agreement (EULA)\n",
        " * associated with this source code for terms and conditions that govern\n",
        " * your use of this NVIDIA software.\n",
        " *\n",
        " */\n",
        "\n",
        "\n",
        "#ifndef __BOOK_H__\n",
        "#define __BOOK_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#endif  // __BOOK_H__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-v3Rm1CRX8O",
        "outputId": "08a67adc-7a17-473a-a618-7bb35e066a3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing book.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <stdio.h>\n",
        "#include \"book.h\"\n",
        "\n",
        "// 3.2.1 Hello, World!\n",
        "/*\n",
        "int main( void ) {\n",
        "  printf( \"Hello, World!\\n\" );\n",
        "  return 0;\n",
        "}\n",
        "*/\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "\n",
        "// 3.2.2 A Kernel Call\n",
        "/*\n",
        "__global__ void kernel(void) {}\n",
        "\n",
        "int main(void) {\n",
        "    kernel<<<1,1>>>();\n",
        "    printf(\"Hello, World!\\n\");\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// NOTES:\n",
        "// - __global__ alerts the complier that a function should be complied to run on\n",
        "//   a device instead of the host\n",
        "// - <<1, 1>> = arguments we plan to pass to the runtime system\n",
        "*/\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "\n",
        "// 3.2.3 Passing Parameters\n",
        "/*\n",
        "// CUDA kernel function defined using __global__\n",
        "__global__ void add(int a, int b, int *c) {\n",
        "    *c = a + b;\n",
        "}\n",
        "\n",
        "// host (CPU) code\n",
        "int main(void) {\n",
        "    int c;\n",
        "    int *dev_c; // points to memory on the device (GPU)\n",
        "\n",
        "    // allocates memory on the device (GPU)\n",
        "    HANDLE_ERROR(cudaMalloc((void**)&dev_c, sizeof(int)));\n",
        "\n",
        "    add<<<1, 1>>>(2, 7, dev_c); // <<<1, 1>>> means one block with one thread\n",
        "\n",
        "    // copies result from the device memory pointed to by dev_c back to the\n",
        "    // host memory location of the variable c\n",
        "    HANDLE_ERROR(cudaMemcpy(&c, dev_c, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    printf(\"2 + 7 = %d\\n\", c);\n",
        "    cudaFree(dev_c); // deallocates the memory on the device (GPU)\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "*/\n",
        "\n",
        "// NOTES:\n",
        "// - cudaMalloc() tells the CUDA runtime to allocate the memory on the device.\n",
        "// - The first argument is a pointer to the pointer you want to hold the address\n",
        "//   of the newly allocated memory, and the second parameter is the size of the\n",
        "//   allocation you want to make.\n",
        "// - It returns a pointer (a memory address) that refers to a location on the\n",
        "//   device.\n",
        "// - RULES:\n",
        "//    - You can pass pointers allocated with cudaMalloc() to functions that\n",
        "//      execute on the device.\n",
        "//    - You can use pointers allocated with cudaMalloc()to read or write\n",
        "//      memory from code that executes on the device.\n",
        "//    - You can pass pointers allocated with cudaMalloc()to functions that\n",
        "//      execute on the host.\n",
        "//    - You cannot use pointers allocated with cudaMalloc()to read or write\n",
        "//      memory from code that executes on the host.\n",
        "// - CPU's memory controller cannot directly access the GPU's memory (!!)\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "// 3.3\n",
        "int main(void) {\n",
        "  cudaDeviceProp prop;\n",
        "\n",
        "  int count;\n",
        "  HANDLE_ERROR(cudaGetDeviceCount(&count));\n",
        "  for (int i = 0; i < count; i++) {\n",
        "    HANDLE_ERROR(cudaGetDeviceProperties(&prop, i));\n",
        "    printf(\" --- General Information for device %d ---\\n\", i);\n",
        "    printf(\"Name: %s\\n\", prop.name);\n",
        "    printf(\"Compute capability: %d.%d\\n\", prop.major, prop.minor);\n",
        "    printf(\"Clock rate: %d\\n\", prop.clockRate);\n",
        "    printf(\"Device copy overlap: \");\n",
        "    if (prop.deviceOverlap)\n",
        "        printf(\"Enabled\\n\");\n",
        "    else\n",
        "        printf(\"Disabled\\n\");\n",
        "    printf(\"Kernel execition timeout : \");\n",
        "    if (prop.kernelExecTimeoutEnabled)\n",
        "        printf(\"Enabled\\n\");\n",
        "    else\n",
        "        printf(\"Disabled\\n\");\n",
        "    printf(\" --- Memory Information for device %d ---\\n\", i);\n",
        "    printf(\"Total global mem: %ld\\n\", prop.totalGlobalMem);\n",
        "    printf(\"Total constant Mem: %ld\\n\", prop.totalConstMem);\n",
        "    printf(\"Max mem pitch: %ld\\n\", prop.memPitch);\n",
        "    printf(\"Texture Alignment: %ld\\n\", prop.textureAlignment);\n",
        "    printf(\" --- MP Information for device %d ---\\n\", i);\n",
        "    printf(\"Multiprocessor count: %d\\n\", prop.multiProcessorCount);\n",
        "    printf(\"Shared mem per mp: %ld\\n\", prop.sharedMemPerBlock);\n",
        "    printf(\"Registers per mp: %d\\n\", prop.regsPerBlock);\n",
        "    printf(\"Threads in warp: %d\\n\", prop.warpSize);\n",
        "    printf(\"Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "    printf(\"Max thread dimensions: (%d, %d, %d)\\n\", prop.maxThreadsDim[0],\n",
        "            prop.maxThreadsDim[1],prop.maxThreadsDim[2]);\n",
        "    printf(\"Max grid dimensions: (%d, %d, %d)\\n\", prop.maxGridSize[0],\n",
        "            prop.maxGridSize[1], prop.maxGridSize[2]);\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCxi04hRK3Rk",
        "outputId": "4caa3916-aa9b-47e0-bfea-99bb2f802fa4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu -o my_cuda_program\n",
        "!./my_cuda_program"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXZRhjNJMiUf",
        "outputId": "6b705d5f-4ddb-4608-c09a-636f48881816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " --- General Information for device 0 ---\n",
            "Name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Clock rate: 1590000\n",
            "Device copy overlap: Enabled\n",
            "Kernel execition timeout : Disabled\n",
            " --- Memory Information for device 0 ---\n",
            "Total global mem: 15828320256\n",
            "Total constant Mem: 65536\n",
            "Max mem pitch: 2147483647\n",
            "Texture Alignment: 512\n",
            " --- MP Information for device 0 ---\n",
            "Multiprocessor count: 40\n",
            "Shared mem per mp: 49152\n",
            "Registers per mp: 65536\n",
            "Threads in warp: 32\n",
            "Max threads per block: 1024\n",
            "Max thread dimensions: (1024, 1024, 64)\n",
            "Max grid dimensions: (2147483647, 65535, 65535)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}