{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMN8ZRk+X2sdG2YC89WatRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caileymm/cuda-by-example-exercises/blob/main/chapter_5_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xenPw-PbJUz_",
        "outputId": "9576476a-bda0-47c5-fda8-e78a8bb45251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp57msb0sa\".\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile book.h\n",
        "\n",
        "#ifndef __BOOK_H__\n",
        "#define __BOOK_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "#endif  // __BOOK_H__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-v3Rm1CRX8O",
        "outputId": "82757ab5-5792-4c66-9934-c51cbd3ec2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting book.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_sums_1.cu\n",
        "\n",
        "// 5.2.1 Vector Sums: Redux\n",
        "// GPU Vector Sums with Threads\n",
        "\n",
        "#include \"book.h\"\n",
        "\n",
        "#define N 10\n",
        "\n",
        "__global__ void add( int *a, int *b, int *c ) {\n",
        "  int tid = threadIdx.x; // index the data by thread index\n",
        "  if (tid < N)\n",
        "    c[tid] = a[tid] + b[tid];\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "  int a[N], b[N], c[N];\n",
        "  int *dev_a, *dev_b, *dev_c;\n",
        "\n",
        "  // allocate the memory on the GPU\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_a, N * sizeof(int)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_b, N * sizeof(int)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_c, N * sizeof(int)));\n",
        "\n",
        "  // fill the arrays ‘a’ and ‘b’ on the CPU\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i * i;\n",
        "  }\n",
        "\n",
        "  // copy the arrays ‘a’ and ‘b’ to the GPU\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice));\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice));\n",
        "  add<<<1,N>>>(dev_a, dev_b, dev_c); // N threads in one block\n",
        "\n",
        "  // copy the array ‘c’ back from the GPU to the CPU\n",
        "  HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // display the results\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
        "  }\n",
        "\n",
        "  // free the memory allocated on the GPU\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "// NOTES:\n",
        "// - Blocks: Groups of threads that execute a kernel in parallel.\n",
        "// - Each block can handle a portion of the data, allowing for efficient\n",
        "// - parallel processing across the GPU.\n",
        "// - <kernel function name><<<N,M>>>(): M is the number of threads per block\n",
        "// - N blocks * M threads/block"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCxi04hRK3Rk",
        "outputId": "7002724d-8420-426d-b316-a0488aab9e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vector_sums_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_sums_1.cu -o vector_sums_1\n",
        "!./vector_sums_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXZRhjNJMiUf",
        "outputId": "6c8a5eba-23d5-46ff-d0d7-faac24ffba8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 + 0 = 0\n",
            "1 + 1 = 0\n",
            "2 + 4 = 0\n",
            "3 + 9 = 0\n",
            "4 + 16 = 0\n",
            "5 + 25 = 0\n",
            "6 + 36 = 0\n",
            "7 + 49 = 0\n",
            "8 + 64 = 0\n",
            "9 + 81 = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_sums_2.cu\n",
        "\n",
        "// 5.2.1 Vector Sums: Redux\n",
        "// GPU Sums OF Arbitrarily Long Vectors\n",
        "\n",
        "# include \"book.h\"\n",
        "# define N 33 * 1024\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "  // calculates id of thread across all blocks in the grid\n",
        "  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  // grid-stride loop\n",
        "  while (tid < N) {\n",
        "    c[tid] = a[tid] + b[tid];\n",
        "    // add total number of threads in the grid to the current tid\n",
        "    tid += blockDim.x * gridDim.x;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "  int a[N], b[N], c[N];\n",
        "  int *dev_a, *dev_b, *dev_c;\n",
        "\n",
        "  // allocate the memory on the GPU\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_a, N * sizeof(int)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_b, N * sizeof(int)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**)&dev_c, N * sizeof(int)));\n",
        "\n",
        "  // fill the arrays ‘a’ and ‘b’ on the CPU\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i * i;\n",
        "  }\n",
        "\n",
        "  // copy the arrays 'a' and 'b' to the GPU\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(int), cudaMemcpyHostToDevice));\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(int), cudaMemcpyHostToDevice));\n",
        "  add<<<128,128>>>(dev_a, dev_b, dev_c); // 128 * 128 = 16384 total threads\n",
        "\n",
        "  // copy the array 'c' back from the GPU to the CPU\n",
        "  HANDLE_ERROR(cudaMemcpy(c, dev_c, N * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // verify that the GPU did the work we requested\n",
        "  bool success = true;\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    if ((a[i] + b[i]) != c[i]) {\n",
        "      // printf(\"Error: %d + %d != %d\\n\", a[i], b[i], c[i]);\n",
        "      success = false;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (success) printf(\"We did it!\\n\");\n",
        "\n",
        "  // free the memory allocated on the GPU\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "  return 0;\n",
        "\n",
        "  // NOTES:\n",
        "  // - threadIdx.x: Thread index within its block in the x-dimension.\n",
        "  // - blockIdx.x: Block index within the grid in the x-dimension.\n",
        "  // - blockDim.x: Number of threads per block in the x-dimension.\n",
        "  // - Grid-stride loop ensures that all elements in the array are processed,\n",
        "  // - even if the number of elements N is much larger than the total number of\n",
        "  // - threads launched.\n",
        "  // - 16384 total threads but N = 33792, so grid-stride loop is necessary\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "693szhHgUdl2",
        "outputId": "002af883-b7f6-4798-b16d-e41f19522413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_sums_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_sums_2.cu -o vector_sums_2\n",
        "!./vector_sums_2"
      ],
      "metadata": {
        "id": "Qck8rxgJVTow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dot_product.cu\n",
        "\n",
        "// 5.3.1 Dot Product\n",
        "\n",
        "#include \"book.h\"\n",
        "\n",
        "#define imin(a, b) (a < b ? a : b)\n",
        "\n",
        "const int N = 33 * 1024; // size of vectors\n",
        "const int threadsPerBlock = 256;\n",
        "const int blocksPerGrid = imin(32, (N + threadsPerBlock - 1) / threadsPerBlock);\n",
        "\n",
        "__global__ void dot(float *a, float *b, float *c) {\n",
        "  __shared__ float cache[threadsPerBlock]; // shared memory per BLOCK\n",
        "  int tid = threadIdx.x + blockIdx.x * blockDim.x; // global thread index\n",
        "  int cacheIndex = threadIdx.x; // local thread index\n",
        "  float temp = 0;\n",
        "\n",
        "  // grid-stride loop\n",
        "  // each thread calculates a sum of products (SUBSET of total products)\n",
        "  while (tid < N) {\n",
        "    temp += a[tid] * b[tid];\n",
        "    tid += blockDim.x * gridDim.x;\n",
        "  }\n",
        "\n",
        "  cache[cacheIndex] = temp; // stores each thread's sum of products\n",
        "\n",
        "  __syncthreads(); // ensures all threads in BLOCK calculated sum of products\n",
        "\n",
        "  // parallel reduction (executed once per block)\n",
        "  // sum of each thread's sum of products\n",
        "  int i = blockDim.x / 2; // half of threads per block\n",
        "  while (i != 0) {\n",
        "    // threads with index below i sum with threads with index above i\n",
        "    if (cacheIndex < i)\n",
        "      cache[cacheIndex] += cache[cacheIndex + i];\n",
        "    __syncthreads(); // ensures all threads within block have completed the addtion in the current iteration\n",
        "    i /= 2;\n",
        "  }\n",
        "\n",
        "  // first thread in block holds sum of all sum of products\n",
        "  if (cacheIndex == 0)\n",
        "    c[blockIdx.x] = cache[0];\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  float *a, *b, c, *partial_c;\n",
        "  float *dev_a, *dev_b, *dev_partial_c;\n",
        "\n",
        "  // allocate memory on the CPU side\n",
        "  a = (float*) malloc(N * sizeof(float));\n",
        "  b = (float*) malloc(N * sizeof(float));\n",
        "  partial_c = (float*) malloc(blocksPerGrid * sizeof(float));\n",
        "\n",
        "  // allocate the memory on the GPU\n",
        "  HANDLE_ERROR(cudaMalloc((void**) &dev_a, N * sizeof(float)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**) &dev_b, N * sizeof(float)));\n",
        "  HANDLE_ERROR(cudaMalloc((void**) &dev_partial_c,blocksPerGrid * sizeof(float)));\n",
        "\n",
        "  // fill in the host memory with data\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    a[i] = i;\n",
        "    b[i] = i * 2;\n",
        "  }\n",
        "\n",
        "  // copy the arrays ‘a’ and ‘b’ to the GPU\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_a, a, N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  HANDLE_ERROR(cudaMemcpy(dev_b, b, N * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dot<<<blocksPerGrid,threadsPerBlock>>>(dev_a, dev_b, dev_partial_c );\n",
        "\n",
        "  // copy the array 'c' back from the GPU to the CPU\n",
        "  HANDLE_ERROR(cudaMemcpy(partial_c, dev_partial_c, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // finish up on the CPU side\n",
        "  c = 0;\n",
        "  for (int i = 0; i < blocksPerGrid; i++) {\n",
        "    c += partial_c[i];\n",
        "  }\n",
        "\n",
        "  #define sum_squares(x) (x * (x + 1) * (2 * x + 1) / 6)\n",
        "\n",
        "  printf(\"Does GPU value %.6g = %.6g?\\n\", c, 2 * sum_squares((float)(N - 1)));\n",
        "\n",
        "  // free memory on the GPU side\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_partial_c);\n",
        "\n",
        "  // free memory on the CPU side\n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(partial_c);\n",
        "}\n",
        "\n",
        "// NOTES:\n",
        "// - (N + threadsPerBlock - 1) / threadsPerBlock is the standard way to\n",
        "//   calculate the number of blocks needed to process N elements, given that\n",
        "//   each block can process threadsPerBlock elements\n",
        "// - if (cacheIndex < i) {\n",
        "//      cache[cacheIndex] += cache[cacheIndex + i];\n",
        "//      __syncthreads();\n",
        "//   }\n",
        "//   This results in a deadlock! The threads that reaches __syncthreads()\n",
        "//   (index < i) will wait indefinitely for threads that skipped it (index >= i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obFt4fkwR435",
        "outputId": "0a6a08ca-f5fb-4854-ddd1-a0b540733114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dot_product.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc dot_product.cu -o dot_product\n",
        "!./dot_product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIViXU8xX1c6",
        "outputId": "57f32ab8-2387-4aa4-da45-ccc9f4e12ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does GPU value 0 = 2.57236e+13?\n"
          ]
        }
      ]
    }
  ]
}