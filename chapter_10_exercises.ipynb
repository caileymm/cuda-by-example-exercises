{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPggvOnB5H3ydD2phCg9Tnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caileymm/cuda-by-example-exercises/blob/main/chapter_10_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile page_locked.cu\n",
        "#include \"book.h\"\n",
        "\n",
        "// 10.2 Page Locked Host memory\n",
        "\n",
        "float cuda_malloc_test( int size, bool up ) {\n",
        "    cudaEvent_t start, stop;\n",
        "    int *a, *dev_a;\n",
        "    float elapsedTime;\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "    a = (int*)malloc( size * sizeof( *a ) );\n",
        "    HANDLE_NULL( a );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a,\n",
        "    size * sizeof( *dev_a ) ) );\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "    for (int i=0; i<100; i++) {\n",
        "        if (up)\n",
        "            HANDLE_ERROR( cudaMemcpy( dev_a, a,\n",
        "            size * sizeof( *dev_a ),\n",
        "            cudaMemcpyHostToDevice ) );\n",
        "        else\n",
        "            HANDLE_ERROR( cudaMemcpy( a, dev_a,\n",
        "            size * sizeof( *dev_a ),\n",
        "            cudaMemcpyDeviceToHost ) );\n",
        "    }\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "    start, stop ) );\n",
        "    free( a );\n",
        "    HANDLE_ERROR( cudaFree( dev_a ) );\n",
        "    HANDLE_ERROR( cudaEventDestroy( start ) );\n",
        "    HANDLE_ERROR( cudaEventDestroy( stop ) );\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "float cuda_host_alloc_test(int size, bool up) {\n",
        "    cudaEvent_t start, stop;\n",
        "    int *a, *dev_a;\n",
        "    float elapsedTime;\n",
        "\n",
        "    HANDLE_ERROR(cudaEventCreate(&start));\n",
        "    HANDLE_ERROR(cudaEventCreate(&stop));\n",
        "    HANDLE_ERROR(cudaHostAlloc((void**)&a, size * sizeof(*a), cudaHostAllocDefault));\n",
        "    HANDLE_ERROR(cudaMalloc((void**)&dev_a, size * sizeof(*dev_a)));\n",
        "    HANDLE_ERROR(cudaEventRecord(start, 0));\n",
        "\n",
        "    for (int i = 0; i < 100; i++) {\n",
        "        if (up)\n",
        "            HANDLE_ERROR(cudaMemcpy(dev_a, a, size * sizeof(*a), cudaMemcpyHostToDevice));\n",
        "        else\n",
        "            HANDLE_ERROR(cudaMemcpy(a, dev_a, size * sizeof(*a), cudaMemcpyDeviceToHost));\n",
        "    }\n",
        "\n",
        "    HANDLE_ERROR(cudaEventRecord(stop, 0));\n",
        "    HANDLE_ERROR(cudaEventSynchronize(stop));\n",
        "    HANDLE_ERROR(cudaEventElapsedTime(&elapsedTime, start, stop));\n",
        "    HANDLE_ERROR(cudaFreeHost(a));\n",
        "    HANDLE_ERROR(cudaFree(dev_a));\n",
        "    HANDLE_ERROR(cudaEventDestroy(start));\n",
        "    HANDLE_ERROR(cudaEventDestroy(stop));\n",
        "\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "#define SIZE (10 * 1024 * 1024)\n",
        "\n",
        "int main(void) {\n",
        "    float elapsedTime;\n",
        "    float MB = (float)100 * SIZE * sizeof(int) / 1024 / 1024;\n",
        "\n",
        "    elapsedTime = cuda_malloc_test(SIZE, true);\n",
        "    printf(\"Time using cudaMalloc: %3.1f ms\\n\", elapsedTime);\n",
        "    printf(\"\\tMB/s during copy up: %3.1f\\n\", MB / (elapsedTime / 1000));\n",
        "\n",
        "    elapsedTime = cuda_malloc_test(SIZE, false);\n",
        "    printf(\"Time using cudaMalloc: %3.1f ms\\n\", elapsedTime);\n",
        "    printf(\"\\tMB/s during copy down: %3.1f\\n\", MB / (elapsedTime / 1000));\n",
        "\n",
        "    elapsedTime = cuda_host_alloc_test(SIZE, true);\n",
        "    printf(\"Time using cudaHostAlloc: %3.1f ms\\n\", elapsedTime);\n",
        "    printf(\"\\tMB/s during copy up: %3.1f\\n\", MB / (elapsedTime / 1000));\n",
        "\n",
        "    elapsedTime = cuda_host_alloc_test(SIZE, false);\n",
        "    printf(\"Time using cudaHostAlloc: %3.1f ms\\n\", elapsedTime);\n",
        "    printf(\"\\tMB/s during copy down: %3.1f\\n\", MB / (elapsedTime / 1000));\n",
        "}\n",
        "\n",
        "// NOTES:\n",
        "// - malloc() allocates pageable host memory, which the OS can page out to disk\n",
        "//   or relocate in physical memory.\n",
        "// - cudaHostAlloc() allocates page-locked (pinned) host memory, which the OS\n",
        "//   guarantees will remain in physical memory (not paged to disk or relocated).\n",
        "// - Pinned memory allows the GPU to use Direct Memory Access (DMA) for faster\n",
        "//   data transfers between host and GPU.\n",
        "// - DMA requires knowing the physical address of the memory, which is stable\n",
        "//   only for pinned memory.\n",
        "// - Without pinned memory, the OS could move or page out data during DMA,\n",
        "//   causing errors.\n",
        "// - If you use malloc() for GPU transfers, the CUDA driver must:\n",
        "//   1. First copy data from pageable memory to a temporary page-locked\n",
        "//      \"staging\" buffer. Then perform DMA from the staging buffer to the GPU.\n",
        "//   2. This double copy slows down transfers compared to directly using pinned\n",
        "//      memory with cudaHostAlloc()."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCxi04hRK3Rk",
        "outputId": "945a03fc-5824-41df-afcc-4013308729d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing page_locked.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc page_locked.cu -o page_locked\n",
        "!./page_locked"
      ],
      "metadata": {
        "id": "lLzrbehARBwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d8ddd3-3404-4106-e235-a8b2fe38dd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time using cudaMalloc: 676.5 ms\n",
            "\tMB/s during copy up: 5912.7\n",
            "Time using cudaMalloc: 1046.5 ms\n",
            "\tMB/s during copy down: 3822.2\n",
            "Time using cudaHostAlloc: 356.9 ms\n",
            "\tMB/s during copy up: 11206.5\n",
            "Time using cudaHostAlloc: 339.4 ms\n",
            "\tMB/s during copy down: 11786.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile single_stream.cu\n",
        "\n",
        "// 10.4 Using Single CUDA Stream\n",
        "\n",
        "#include \"book.h\"\n",
        "#define N (1024 * 1024)\n",
        "#define FULL_DATA_SIZE (N * 20)\n",
        "\n",
        "// Kernel to perform some computation on the input arrays\n",
        "__global__ void kernel( int *a, int *b, int *c ) {\n",
        "    // Calculate the global thread index\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    // Check array bounds\n",
        "    if (idx < N) {\n",
        "        // Perform a simple calculation using neighboring elements\n",
        "        int idx1 = (idx + 1) % 256;\n",
        "        int idx2 = (idx + 2) % 256;\n",
        "        float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;\n",
        "        float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;\n",
        "        c[idx] = (as + bs) / 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "    cudaDeviceProp prop;\n",
        "    int whichDevice;\n",
        "\n",
        "    // Get the current device and its properties\n",
        "    HANDLE_ERROR( cudaGetDevice( &whichDevice ) );\n",
        "    HANDLE_ERROR( cudaGetDeviceProperties( &prop, whichDevice ) );\n",
        "\n",
        "    // Check if the device supports overlapping copy and kernel execution\n",
        "    if (!prop.deviceOverlap) {\n",
        "        printf( \"Device will not handle overlaps, so no \"\n",
        "        \"speed up from streams\\n\" );\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    float elapsedTime;\n",
        "\n",
        "    // Create CUDA events for timing the execution\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "    // Record the start event\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "\n",
        "    // Create a CUDA stream\n",
        "    cudaStream_t stream;\n",
        "    HANDLE_ERROR( cudaStreamCreate( &stream ) );\n",
        "\n",
        "    int *host_a, *host_b, *host_c; // Host memory pointers\n",
        "    int *dev_a, *dev_b, *dev_c; // Device memory pointers\n",
        "\n",
        "    // Allocate memory on the GPU\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a, N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b, N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c, N * sizeof(int) ) );\n",
        "\n",
        "    // Allocate page-locked (pinned) memory on the host for faster async transfers\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_a, FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_b, FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_c, FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    // Initialize host data with random values\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i++) {\n",
        "        host_a[i] = rand();\n",
        "        host_b[i] = rand();\n",
        "    }\n",
        "\n",
        "    // Process the data in chunks to demonstrate streaming\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i+= N) {\n",
        "        // Asynchronously copy data from host (page-locked) to device\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_a, host_a+i, N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice, stream ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_b, host_b+i, N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice, stream ) );\n",
        "\n",
        "        // Launch the kernel on the stream. It will execute after the above copies complete.\n",
        "        kernel<<<N/256,256,0,stream>>>( dev_a, dev_b, dev_c );\n",
        "\n",
        "        // Asynchronously copy the result from device back to host (page-locked)\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( host_c+i, dev_c, N * sizeof(int),\n",
        "        cudaMemcpyDeviceToHost, stream ) );\n",
        "    }\n",
        "\n",
        "    // Wait for all operations in the stream to complete\n",
        "    HANDLE_ERROR( cudaStreamSynchronize( stream ) );\n",
        "\n",
        "    // Record the stop event and calculate the elapsed time\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime, start, stop ) );\n",
        "    printf( \"Time taken: %3.1f ms\\n\", elapsedTime );\n",
        "\n",
        "    // Clean up: free host and device memory\n",
        "    HANDLE_ERROR( cudaFreeHost( host_a ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_b ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_c ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_a ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_b ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_c ) );\n",
        "\n",
        "    // Destroy the stream\n",
        "    HANDLE_ERROR( cudaStreamDestroy( stream ) );\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// NOTES:\n",
        "// - A stream is essentially a sequence of operations that are executed in\n",
        "//   order on the GPU (like a queue). The GPU will then execute these commands\n",
        "//   in the order they were added to the stream.\n",
        "// - cudaMemcpyAsync enables asynchronous data transfers, which means the CPU\n",
        "//   can initiate a data copy and then immediately move on to other tasks\n",
        "//   without waiting for the copy to complete. This allows for the overlapping\n",
        "//   of data transfers and computation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zem1B9k70qZH",
        "outputId": "6c3c6a55-bbf0-4c93-d858-0c100eb77c9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing single_stream.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc single_stream.cu -o single_stream\n",
        "!./single_stream"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvMATtvR1GrC",
        "outputId": "871ac056-9811-4fe0-f769-8ae4faa8befa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 1075.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multiple_stream.cu\n",
        "\n",
        "// 10.5 Using Multiple CUDA Streams\n",
        "\n",
        "#include \"book.h\"\n",
        "#define N (1024*1024)\n",
        "#define FULL_DATA_SIZE (N*20)\n",
        "__global__ void kernel( int *a, int *b, int *c ) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < N) {\n",
        "        int idx1 = (idx + 1) % 256;\n",
        "        int idx2 = (idx + 2) % 256;\n",
        "        float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;\n",
        "        float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;\n",
        "        c[idx] = (as + bs) / 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "    cudaDeviceProp prop;\n",
        "    int whichDevice;\n",
        "    HANDLE_ERROR( cudaGetDevice( &whichDevice ) );\n",
        "    HANDLE_ERROR( cudaGetDeviceProperties( &prop, whichDevice ) );\n",
        "    if (!prop.deviceOverlap) {\n",
        "        printf( \"Device will not handle overlaps, so no \"\n",
        "        \"speed up from streams\\n\" );\n",
        "        return 0;\n",
        "    }\n",
        "    cudaEvent_t start, stop;\n",
        "    float elapsedTime;\n",
        "    // start the timers\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "\n",
        "    // initialize the streams\n",
        "    cudaStream_t stream0, stream1;\n",
        "    HANDLE_ERROR( cudaStreamCreate( &stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamCreate( &stream1 ) );\n",
        "\n",
        "    int *host_a, *host_b, *host_c;\n",
        "    int *dev_a0, *dev_b0, *dev_c0; //GPU buffers for stream0\n",
        "    int *dev_a1, *dev_b1, *dev_c1; //GPU buffers for stream1\n",
        "    // allocate the memory on the GPU\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a1,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b1,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c1,\n",
        "    N * sizeof(int) ) );\n",
        "    // allocate page-locked memory, used to stream\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_a,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_b,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_c,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i++) {\n",
        "        host_a[i] = rand();\n",
        "        host_b[i] = rand();\n",
        "    }\n",
        "    // now loop over full data, in bite-sized chunks\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i+= N*2) {\n",
        "        // copy the locked memory to the device, async\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_a0, host_a+i,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream0 ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_b0, host_b+i,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream0 ) );\n",
        "        kernel<<<N/256,256,0,stream0>>>( dev_a0, dev_b0, dev_c0 );\n",
        "        // copy the data from device to locked memory\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( host_c+i, dev_c0,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyDeviceToHost,\n",
        "        stream0 ) );\n",
        "        // copy the locked memory to the device, async\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_a1, host_a+i+N,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream1 ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_b1, host_b+i+N,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream1 ) );\n",
        "        kernel<<<N/256,256,0,stream1>>>( dev_a1, dev_b1, dev_c1 );\n",
        "        // copy the data from device to locked memory\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( host_c+i+N, dev_c1,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyDeviceToHost,\n",
        "        stream1 ) );\n",
        "    }\n",
        "\n",
        "    HANDLE_ERROR( cudaStreamSynchronize( stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamSynchronize( stream1 ) );\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "    start, stop ) );\n",
        "    printf( \"Time taken: %3.1f ms\\n\", elapsedTime );\n",
        "    // cleanup the streams and memory\n",
        "    HANDLE_ERROR( cudaFreeHost( host_a ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_b ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_c ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_a0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_b0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_c0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_a1 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_b1 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_c1 ) );\n",
        "    HANDLE_ERROR( cudaStreamDestroy( stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamDestroy( stream1 ) );\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWO02P_r1Kvz",
        "outputId": "a9ba71cb-e9cd-4835-97fb-8b20871f90bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting multiple_stream.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc multiple_stream.cu -o multiple_stream\n",
        "!./multiple_stream"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrQBjtDM2yWI",
        "outputId": "a9f0cc09-0344-4041-8d5d-07bf996f6da9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 858.2 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multiple_stream_efficient.cu\n",
        "\n",
        "// 10.7 Using Multiple CUDA Streams Efficiently\n",
        "\n",
        "#include \"book.h\"\n",
        "#define N (1024*1024)\n",
        "#define FULL_DATA_SIZE (N*20)\n",
        "__global__ void kernel( int *a, int *b, int *c ) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < N) {\n",
        "        int idx1 = (idx + 1) % 256;\n",
        "        int idx2 = (idx + 2) % 256;\n",
        "        float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;\n",
        "        float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;\n",
        "        c[idx] = (as + bs) / 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "    cudaDeviceProp prop;\n",
        "    int whichDevice;\n",
        "    HANDLE_ERROR( cudaGetDevice( &whichDevice ) );\n",
        "    HANDLE_ERROR( cudaGetDeviceProperties( &prop, whichDevice ) );\n",
        "    if (!prop.deviceOverlap) {\n",
        "        printf( \"Device will not handle overlaps, so no \"\n",
        "        \"speed up from streams\\n\" );\n",
        "        return 0;\n",
        "    }\n",
        "    cudaEvent_t start, stop;\n",
        "    float elapsedTime;\n",
        "    // start the timers\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "\n",
        "    // initialize the streams\n",
        "    cudaStream_t stream0, stream1;\n",
        "    HANDLE_ERROR( cudaStreamCreate( &stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamCreate( &stream1 ) );\n",
        "\n",
        "    int *host_a, *host_b, *host_c;\n",
        "    int *dev_a0, *dev_b0, *dev_c0; //GPU buffers for stream0\n",
        "    int *dev_a1, *dev_b1, *dev_c1; //GPU buffers for stream1\n",
        "    // allocate the memory on the GPU\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c0,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_a1,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_b1,\n",
        "    N * sizeof(int) ) );\n",
        "    HANDLE_ERROR( cudaMalloc( (void**)&dev_c1,\n",
        "    N * sizeof(int) ) );\n",
        "    // allocate page-locked memory, used to stream\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_a,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_b,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    HANDLE_ERROR( cudaHostAlloc( (void**)&host_c,\n",
        "    FULL_DATA_SIZE * sizeof(int),\n",
        "    cudaHostAllocDefault ) );\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i++) {\n",
        "        host_a[i] = rand();\n",
        "        host_b[i] = rand();\n",
        "    }\n",
        "\n",
        "    for (int i=0; i<FULL_DATA_SIZE; i+= N*2) {\n",
        "        // enqueue copies of a in stream0 and stream1\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_a0, host_a+i,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream0 ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_a1, host_a+i+N,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream1 ) );\n",
        "        // enqueue copies of b in stream0 and stream1\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_b0, host_b+i,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream0 ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( dev_b1, host_b+i+N,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyHostToDevice,\n",
        "        stream1 ) );\n",
        "        // enqueue kernels in stream0 and stream1\n",
        "        kernel<<<N/256,256,0,stream0>>>( dev_a0, dev_b0, dev_c0 );\n",
        "        kernel<<<N/256,256,0,stream1>>>( dev_a1, dev_b1, dev_c1 );\n",
        "        // enqueue copies of c from device to locked memory\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( host_c+i, dev_c0,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyDeviceToHost,\n",
        "        stream0 ) );\n",
        "        HANDLE_ERROR( cudaMemcpyAsync( host_c+i+N, dev_c1,\n",
        "        N * sizeof(int),\n",
        "        cudaMemcpyDeviceToHost,\n",
        "        stream1 ) );\n",
        "    }\n",
        "\n",
        "    HANDLE_ERROR( cudaStreamSynchronize( stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamSynchronize( stream1 ) );\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "    start, stop ) );\n",
        "    printf( \"Time taken: %3.1f ms\\n\", elapsedTime );\n",
        "    // cleanup the streams and memory\n",
        "    HANDLE_ERROR( cudaFreeHost( host_a ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_b ) );\n",
        "    HANDLE_ERROR( cudaFreeHost( host_c ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_a0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_b0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_c0 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_a1 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_b1 ) );\n",
        "    HANDLE_ERROR( cudaFree( dev_c1 ) );\n",
        "    HANDLE_ERROR( cudaStreamDestroy( stream0 ) );\n",
        "    HANDLE_ERROR( cudaStreamDestroy( stream1 ) );\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEccc4l42EDW",
        "outputId": "1345c73b-aab7-4eda-df93-38de62ce8795"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting multiple_stream_efficient.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc multiple_stream_efficient.cu -o multiple_stream_efficient\n",
        "!./multiple_stream_efficient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHquBCVh23F0",
        "outputId": "8c76cdb7-3db6-41dc-a55f-102d43cb6d2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 855.5 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xenPw-PbJUz_",
        "outputId": "f3c16c74-681b-4744-b086-83abfb68a8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpk0vzrux_\".\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile book.h\n",
        "\n",
        "#ifndef __BOOK_H__\n",
        "#define __BOOK_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "#endif  // __BOOK_H__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-v3Rm1CRX8O",
        "outputId": "e9f66493-4c89-4fbd-9d2a-4632c4eb2f94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing book.h\n"
          ]
        }
      ]
    }
  ]
}